{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸï¸ LossZero: Motorcycle Night Ride SegFormer-B2 Optimized\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ **SegFormer-B2** ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì•¼ê°„ ì˜¤í† ë°”ì´ ì£¼í–‰ ì´ë¯¸ì§€ì˜ ì‹œë©˜í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ› ï¸ ì£¼ìš” ì‹œë‚˜ë¦¬ì˜¤\n",
    "- **ëª¨ë¸**: SegFormer-B2 (Transformer ê¸°ë°˜)\n",
    "- **ë°±ë³¸**: MiT-B2\n",
    "- **ì‚¬ì „ í•™ìŠµ**: Cityscapes (ë„ë¡œ í™˜ê²½ íŠ¹í™”)\n",
    "- **ìµœì í™”**: AdamW + FP16 Mixed Precision\n",
    "- **ì†ì‹¤ í•¨ìˆ˜**: Weighted CrossEntropy (ì¤‘ìš” ê°ì²´ ê°€ì¤‘ì¹˜ ë¶€ì—¬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "setup",
    "outputId": "f6613083-91f9-4ae4-ffd9-c389c34b61ae"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "8w7h3L2Ys1pf"
   },
   "source": [
    "## Colab ì—°ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab-mount",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "config",
    "outputId": "ba37f54f-5fb9-418d-f013-6c95f4790c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Local Environment\n",
      "Using device: cpu\n",
      "Data directory: /Users/jamesyang/Projects/LossZero/data/Motorcycle Night Ride Dataset\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "\n",
    "    return \"cpu\"\n",
    "\n",
    "def num_worker():\n",
    "    if torch.cuda.is_available():\n",
    "        return os.cpu_count()\n",
    "\n",
    "    return 0\n",
    "\n",
    "# âš™ï¸ ì„¤ì • (Configuration)\n",
    "#DATA_DIR = \"/content/drive/MyDrive/motor_model\"\n",
    "DATA_DIR = os.path.expanduser(\"~/Projects/LossZero/data/Motorcycle Night Ride Dataset\")\n",
    "print(\"Detected Local Environment\")\n",
    "\n",
    "JSON_PATH = os.path.join(DATA_DIR, \"COCO_motorcycle (pixel).json\")\n",
    "IMG_DIR = os.path.join(DATA_DIR, \"images\")\n",
    "\n",
    "CFG = {\n",
    "    \"project\": \"LossZero\",\n",
    "    \"model_name\": \"nvidia/segformer-b2-finetuned-cityscapes-1024-1024\",\n",
    "    \"img_size\": (352, 352),\n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"epochs\": 25,\n",
    "    \"device\": get_device(),\n",
    "    \"num_worker\": num_worker()\n",
    "}\n",
    "\n",
    "print(f\"Using device: {CFG['device']}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dataset",
    "outputId": "a175932e-0de9-4f41-f50f-dcdf808db85c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesyang/.pyenv/versions/3.12.2/envs/aipel/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.91s)\n",
      "creating index...\n",
      "index created!\n",
      "Category Mapping: {1329681: 0, 1323885: 1, 1323884: 2, 1323882: 3, 1323881: 4, 1323880: 5}\n"
     ]
    }
   ],
   "source": [
    "def create_mask_from_json(coco, img_id, img_info, id_to_idx):\n",
    "    ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    anns = coco.loadAnns(ann_ids)\n",
    "    mask = np.zeros((img_info['height'], img_info['width']), dtype=np.uint8)\n",
    "\n",
    "    for ann in anns:\n",
    "        cat_id = ann['category_id']\n",
    "        if cat_id in id_to_idx:\n",
    "            cls_idx = id_to_idx[cat_id]\n",
    "            pixel_mask = coco.annToMask(ann)\n",
    "            mask[pixel_mask == 1] = cls_idx\n",
    "\n",
    "    return mask\n",
    "\n",
    "def process_single_data(coco, img_id, img_dir, id_to_idx, transform=None):\n",
    "    img_info = coco.loadImgs(img_id)[0]\n",
    "    img_path = os.path.join(img_dir, img_info['file_name'])\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    mask = create_mask_from_json(coco, img_id, img_info, id_to_idx)\n",
    "\n",
    "    if transform:\n",
    "        augmented = transform(image=image, mask=mask)\n",
    "        image, mask = augmented['image'], augmented['mask']\n",
    "\n",
    "    return image, torch.as_tensor(mask).long()\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    #  ì›ë³¸ í•´ìƒë„ì—ì„œ 352x352 í¬ê¸°ë¡œ ë¬´ì‘ìœ„ ì¶”ì¶œ (í™”ì§ˆ ì €í•˜ ì—†ìŒ)\n",
    "    A.RandomCrop(height=CFG['img_size'][0], width=CFG['img_size'][1], p=1.0),\n",
    "    A.PadIfNeeded(min_height=CFG['img_size'][0], min_width=CFG['img_size'][1], p=1.0),\n",
    "\n",
    "    # --- ì•¼ê°„ ì „ìš© Augmentation ì¶”ê°€ ---\n",
    "    A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.RandomGamma(gamma_limit=(80, 120), p=0.5), # ì–´ë‘ìš´ ì €ì¡°ë„ ê°œì„ \n",
    "    A.GaussNoise(std_range=(0.02, 0.05), p=0.3), # ì•¼ê°„ ë…¸ì´ì¦ˆ ëŒ€ì‘\n",
    "\n",
    "    # --- ê¸°í•˜í•™ì  ë³€í˜• (ë°ì´í„° ìˆ˜ ë³´ì¶©ìš©) ---\n",
    "    A.HorizontalFlip(p=0.5), # ì¢Œìš° ë°˜ì „\n",
    "    # 0.0625ëŠ” ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì˜¤ë«ë™ì•ˆ ê²€ì¦ëœ 'ì‚¬ì‹¤ìƒ í‘œì¤€(De Facto Standard)\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5), # ì´ë™/í¬ê¸°/íšŒì „\n",
    "\n",
    "    # ImageNet ë°ì´íƒ€ì…‹ì˜ í‰ê· ê°’ ë‚˜ì˜ì§€ ì•ŠìŒ. SegFormerê°€ ImageNet/Cityscapesë¡œ ë°°ì› ìœ¼ë‹ˆê¹Œ\n",
    "    # ëª¨ë¸ì´ ìƒˆë¡œìš´ ì‚¬ì§„ì„ ë°›ì„ ë•Œ: ì…ë ¥_ì´ë¯¸ì§€ = (ì›ë³¸_ì´ë¯¸ì§€ - í‰ê· ) / í‘œì¤€í¸ì°¨\n",
    "    # ì´ë ‡ê²Œ ê³„ì‚°í•´ì£¼ë©´, ì–´ë–¤ ì‚¬ì§„ì´ ë“¤ì–´ì™€ë„ \"í‰ê· ì´ 0ì´ê³  í‘œì¤€í¸ì°¨ê°€ 1ì¸(Standard Normal Distribution)\" ì•„ì£¼ ì˜ˆìœ ë°ì´í„°ë¡œ ë³€ì‹ \n",
    "    # ì „ì²´ ì•¼ê°„ ë°ì´í„°ì…‹ì˜ Mean/Stdë¥¼ ì§ì ‘ ê³„ì‚°í•œ ê°’\n",
    "    A.Normalize(mean=(0.281, 0.268, 0.346), std=(0.347, 0.290, 0.292)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "coco = COCO(JSON_PATH)\n",
    "img_ids = list(coco.imgs.keys())\n",
    "cat_ids = coco.getCatIds()\n",
    "id_to_idx = {cat_id: i for i, cat_id in enumerate(cat_ids)}\n",
    "print(f\"Category Mapping: {id_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "PaxYL05itR0D"
   },
   "source": [
    "## Traing / Val ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSc3mNS_tRHF",
    "outputId": "47fb854a-41eb-4714-9bd3-7ebbf73847ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.85s)\n",
      "creating index...\n",
      "index created!\n",
      "âœ… Data Ready: Train=160, Val=40\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MotorcycleNightRideDataset(Dataset):\n",
    "    def __init__(self, coco, img_ids, img_dir, id_to_idx, transform=None):\n",
    "        self.coco = coco\n",
    "        self.img_ids = img_ids\n",
    "        self.img_dir = img_dir\n",
    "        self.id_to_idx = id_to_idx\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        image, mask = process_single_data(self.coco, img_id, self.img_dir, self.id_to_idx, self.transform)\n",
    "        return image, mask\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ ë° ID ë¶„í•  (8:2)\n",
    "coco = COCO(JSON_PATH)\n",
    "all_ids = list(coco.imgs.keys())\n",
    "train_ids, val_ids = train_test_split(all_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Transform ì •ì˜ (ê¸°ì¡´ ì •ì˜ í™œìš© ë° Valìš© ì¶”ê°€)\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(CFG['img_size'][0], CFG['img_size'][1]),\n",
    "    A.Normalize(mean=(0.281, 0.268, 0.346), std=(0.347, 0.290, 0.292)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# 3. ë°ì´í„°ì…‹ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "train_dataset = MotorcycleNightRideDataset(coco, train_ids, IMG_DIR, id_to_idx, train_transform)\n",
    "val_dataset = MotorcycleNightRideDataset(coco, val_ids, IMG_DIR, id_to_idx, val_transform)\n",
    "\n",
    "# 4. ë°ì´í„° ë¡œë” ìƒì„±\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CFG['batch_size'], \n",
    "    shuffle=True, \n",
    "    num_workers=CFG['num_worker'],\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CFG['batch_size'], \n",
    "    shuffle=False, \n",
    "    num_workers=CFG['num_worker'],\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data Ready: Train={len(train_ids)}, Val={len(val_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "17ecb67c"
   },
   "source": [
    "### ğŸ“‰ í´ë˜ìŠ¤ë³„ ë¶„í¬ ìš”ì•½ (ë‚´ë¦¼ì°¨ìˆœ)\n",
    "\n",
    "1. **Undrivable (ì£¼í–‰ ë¶ˆê°€ ì˜ì—­)**: **42.9%** (ì••ë„ì  1ìœ„)\n",
    "   - ë°°ê²½(í•˜ëŠ˜, ê±´ë¬¼, í’€ìˆ² ë“±)ì´ ì´ë¯¸ì§€ì˜ ì ˆë°˜ ê°€ê¹Œì´ ì°¨ì§€í•©ë‹ˆë‹¤.\n",
    "2. **Road (ì£¼í–‰ ê°€ëŠ¥ ë„ë¡œ)**: **27.1%**\n",
    "   - ë„ë¡œ ìì²´ë„ ê½¤ ë§ì€ ì˜ì—­ì„ ì°¨ì§€í•©ë‹ˆë‹¤.\n",
    "3. **My bike (ë‚´ ì˜¤í† ë°”ì´)**: **15.8%**\n",
    "   - ì£¼í–‰ì ì‹œì ì´ë¼ ë‚´ ì˜¤í† ë°”ì´ê°€ í•­ìƒ ë³´ì´ê¸° ë•Œë¬¸ì— ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "4. **Rider (íƒ‘ìŠ¹ì)**: **8.1%**\n",
    "   - ë‹¤ë¥¸ ì˜¤í† ë°”ì´ ìš´ì „ìë‚˜ ë‚´ ì‹ ì²´ê°€ í¬í•¨ëœ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n",
    "5. **Moveable (ì´ë™ ë¬¼ì²´)**: **4.7%**\n",
    "   - ë‹¤ë¥¸ ì°¨ëŸ‰, ë³´í–‰ì ë“± ì•ˆì „ì— ê°€ì¥ ì¤‘ìš”í•œ ì¥ì• ë¬¼ì¸ë° ë¹„ìœ¨ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤.\n",
    "6. **Lane Mark (ì°¨ì„ )**: **1.4%**\n",
    "   - ê°€ì¥ ì‹¬ê°í•œ ë¶ˆê· í˜•ì…ë‹ˆë‹¤. ë„ë¡œ ì£¼í–‰ì˜ í•µì‹¬ì¸ ì°¨ì„ ì´ ê³ ì‘ 1% ë‚¨ì§“ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471,
     "referenced_widgets": [
      "c769d5b2aa2e409094ca9b8759d59878",
      "2fc4593323a54bd0862fcf085f9274e6",
      "ea1a830623894e6eabbf21b81068fb12",
      "bd4f1f661e6f4493867170d3d10467d3",
      "707a3a3a181b428a9afc41e08821e3a6",
      "1ae5409bf8e143a3bf93cba26842db35",
      "51686477dc9e4e4e9747f3d6deb3b973",
      "dc6e0ac79b4c4b8e8a2414e76e26a01d",
      "7e913f3f41ef499db743e20f49a7806d",
      "ae9e5e3b4398452aba901bf6cc6d785b",
      "66b94080882a4f40b0b45d5bdd92bf9d",
      "0722ecf90228429cb48ff257c31019d0",
      "4bb2e1505f7b48f7844688504bcbf27f",
      "8463b45fff27433195cc69720d82a211",
      "4bd4dec582d747bd8bd36eb802659c35",
      "5d76b397f5204171a87f6199bdb2f2c5",
      "715a11b1946942bd960b232de28e9fb6",
      "ec6ad001af4d4f5d8b0b25da92f993e8",
      "39ef57594bdb4733bda5127d33032fe3",
      "01a7b8b25fd64001bce07ea9fa5c3b22",
      "7eb7a77d94db4ed9bc1779c59fabda55",
      "c940f72e230f48a29a580dd1fcc4b7fd",
      "af1a70aba6f849da907b39bd050bfe21",
      "8373f2b9779c4b5198049d2276972561",
      "def6a1c78a6e459aad4d960d57b85d5a",
      "d31d800bd26a4c26af67b2d5c458c896",
      "450035d2a49d4ffeb459dedd44d47fe5",
      "a89fcf88404d4a6c8f08d731e1435cbc",
      "a31d02bb1cf8476692ac622fd396b9af",
      "cd48a0018be94fe88fe7c131087a7665",
      "4d2b3c4823314818b8f487f10be04ebf",
      "362d6931e3fe4cc3aca150afabd62470",
      "df9edefb936642dbb1de1d02cf1ab77b",
      "c2f74793ca674c16bc4fb569bd3c9540",
      "d743b96cdc154b3f8408f1813feb34be",
      "2fd3990c654c41b3a22892ed31289705",
      "7f86c1e38d99411eb53f2ccf820db1ec",
      "893b98c1c3af407387cd5f9085e3eee0",
      "abf809ed0ec542bbb05c9baaae297054",
      "34452e54c4de4ccdbd559c05548dfac0",
      "15e9394e60c5444694902320034b8912",
      "53ad725ad4524844b798943257460345",
      "514374788cc44dbd804139c7449a18c0",
      "d8a3f9b5583a4c19a68d873361fe0ca8"
     ]
    },
    "id": "model",
    "outputId": "e0bd0c54-4356-4c39-e376-6bb72e291d9e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef80905ae0c4db1ac32f6bd38b7a6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/380 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mSegformerForSemanticSegmentation LOAD REPORT\u001b[0m from: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n",
      "Key                           | Status   |                                                                                                    \n",
      "------------------------------+----------+----------------------------------------------------------------------------------------------------\n",
      "decode_head.classifier.bias   | MISMATCH | Reinit due to size mismatch - ckpt: torch.Size([19]) vs model:torch.Size([6])                      \n",
      "decode_head.classifier.weight | MISMATCH | Reinit due to size mismatch - ckpt: torch.Size([19, 768, 1, 1]) vs model:torch.Size([6, 768, 1, 1])\n",
      "\n",
      "\u001b[3mNotes:\n",
      "- MISMATCH\u001b[3m\t:ckpt weights were loaded, but they did not match the original empty weight shapes.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "id2label = {i: coco.loadCats(cat_id)[0]['name'] for cat_id, i in id_to_idx.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    CFG['model_name'],\n",
    "    num_labels=len(id_to_idx),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(CFG['device'])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=0.01)\n",
    "\n",
    "# âš–ï¸ í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì„¤ì • (Class Weights)\n",
    "weights = torch.tensor([\n",
    "    5.0,   # Rider (8.1%) -> ì ë‹¹íˆ ë†’ì„\n",
    "    2.0,   # My bike (15.8%) -> ë‚®ì¶¤ (ì´ë¯¸ ë§ìŒ)\n",
    "    10.0,  # Moveable (4.7%) -> ê°•ë ¥í•˜ê²Œ ë†’ì„\n",
    "    20.0,  # Lane Mark (1.4%) -> ì•„ì£¼ ê°•ë ¥í•˜ê²Œ!! (í•µì‹¬)\n",
    "    1.0,   # Road (27.1%) -> ê¸°ë³¸\n",
    "    0.5    # Undrivable (42.9%) -> ë‚®ì¶¤ (ë„ˆë¬´ ë§ì•„ì„œ ë°©í•´ë¨)\n",
    "], dtype=torch.float).to(CFG['device'])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "scaler = GradScaler('cuda') if CFG['device'] == 'cuda' else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517,
     "referenced_widgets": [
      "eacd895341274ca083d8ddc1a3879ba4",
      "e35f377246f341738008871078631420",
      "fcb2f2354f2144d4929fc3857b63f00f",
      "70a7f999804b4a008a6d9790a4f39f3a",
      "7a53a9c0e8fe4f599487170344613431",
      "4162fa761b114e70b3db0e6ac5b477dc",
      "a945ecf5247846cc8e3158bae8601127",
      "2ea75b126d1a414cba6807404a86fc1f",
      "a501ab8a0ab14f76b28c83fbfd1d10aa",
      "a51dbb649b514185a751f7d012138fbf",
      "e9bd4700a6ef4cf9b2ff6ed5671ac8a8"
     ]
    },
    "id": "train",
    "outputId": "4e8fc070-2386-4158-8ab7-ccb951632f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ SegFormer-B2 Training Start...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc5f04de41f4d228c64b683f02743fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Epoch [1/25] Train Loss: 1.6318 | Val Loss: 1.5289\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346406528b5d42edb3623ac3b971eb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Train]:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"ğŸš€ SegFormer-B2 Training Start...\")\n",
    "\n",
    "for epoch in range(CFG['epochs']):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    train_loss_sum = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n",
    "    \n",
    "    for images, masks in pbar:\n",
    "        X = images.to(CFG['device']).contiguous()\n",
    "        y = masks.to(CFG['device']).contiguous()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision ì§€ì› (CUDA ì „ìš©)\n",
    "        if CFG['device'] == 'cuda' and scaler:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(X).logits\n",
    "                upsampled_logits = nn.functional.interpolate(outputs, size=y.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                loss = criterion(upsampled_logits, y)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            outputs = model(X).logits\n",
    "            upsampled_logits = nn.functional.interpolate(outputs, size=y.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            loss = criterion(upsampled_logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        train_loss_sum += loss.item()\n",
    "        pbar.set_postfix(Loss=f\"{loss.item():.4f}\")\n",
    "    \n",
    "    avg_train_loss = train_loss_sum / len(train_loader)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            X = images.to(CFG['device']).contiguous()\n",
    "            y = masks.to(CFG['device']).contiguous()\n",
    "            \n",
    "            outputs = model(X).logits\n",
    "            upsampled_logits = nn.functional.interpolate(outputs, size=y.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            loss = criterion(upsampled_logits, y)\n",
    "            val_loss_sum += loss.item()\n",
    "            \n",
    "    avg_val_loss = val_loss_sum / len(val_loader)\n",
    "    print(f\"ğŸ“ Epoch [{epoch+1}/{CFG['epochs']}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71022e54",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ ì£¼ìš” í‰ê°€ í•­ëª©\n",
    "- **mIoU** (Mean Intersection over Union):\n",
    "  - **Category-specific IoU**\n",
    "  - **Boundary IoU**\n",
    "- **ì‹¤ì‹œê°„ì„± ë° í•˜ë“œì›¨ì–´ ì§€í‘œ**\n",
    "  - **Model Parameters**\n",
    "  - **MACs** Multiply-Accumulate Operations\n",
    "    - y = wx + b ì—ì„œ wx + bë¥¼ 1MAC ì´ë¼ê³  í•œë‹¤.\n",
    "  - **GFLOPs** Giga Floating Point Operations\n",
    "    -  ëª¨ë¸ì„ í•œ ë²ˆ ì‹¤í–‰(Forward Pass)í•  ë•Œ í•„ìš”í•œ ì´ ë¶€ë™ ì†Œìˆ˜ì  ì—°ì‚°ëŸ‰\n",
    "    - ë³´í†µ 1MAC = 2FLOPs\n",
    "  - **Average Inference Latency**\n",
    "  - **Frames Per Second (FPS)**\n",
    "- **Safety-critical Metrics**\n",
    "\n",
    "- TODO\n",
    "  - í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¡œ í‰ê°€í•  ê²ƒ ( í˜„ì¬ëŠ” ì „ì²´ ì´ë¯¸ì§€ SET ì‚¬ìš© )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47604556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def compute_category_iou(preds, targets, num_classes):\n",
    "    \"\"\"\n",
    "    Confusion Matrixë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ë³„ IoU(Intersection over Union)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    Args:\n",
    "        preds (torch.Tensor): ì˜ˆì¸¡ëœ ë§ˆìŠ¤í¬ (N, H, W).\n",
    "        targets (torch.Tensor): ì‹¤ì œ ì •ë‹µ(Ground truth) ë§ˆìŠ¤í¬ (N, H, W).\n",
    "        num_classes (int): í´ë˜ìŠ¤ ê°œìˆ˜.\n",
    "    Returns:\n",
    "        np.ndarray: ê° í´ë˜ìŠ¤ë³„ IoU ê°’.\n",
    "    \"\"\"\n",
    "    preds_flat = preds.flatten().cpu().numpy()\n",
    "    targets_flat = targets.flatten().cpu().numpy()\n",
    "\n",
    "    # ìœ íš¨í•˜ì§€ ì•Šì€ íƒ€ê²Ÿ ê°’ ì œì™¸ (ì˜ˆ: íŒ¨ë”© ë˜ëŠ” ignore_indexê°€ í¬í•¨ëœ ê²½ìš°)\n",
    "    valid_mask = (targets_flat >= 0) & (targets_flat < num_classes)\n",
    "    preds_flat = preds_flat[valid_mask]\n",
    "    targets_flat = targets_flat[valid_mask]\n",
    "\n",
    "    # í˜¼ë™ í–‰ë ¬(Confusion Matrix) ê³„ì‚°\n",
    "    cm = confusion_matrix(targets_flat, preds_flat, labels=range(num_classes))\n",
    "\n",
    "    # í´ë˜ìŠ¤ë³„ IoU ê³„ì‚° ê³µì‹ = TP / (TP + FP + FN)\n",
    "    intersection = np.diag(cm)\n",
    "    ground_truth_set = cm.sum(axis=1)\n",
    "    predicted_set = cm.sum(axis=0)\n",
    "    union = ground_truth_set + predicted_set - intersection\n",
    "\n",
    "    # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì˜¤ë¥˜(division by zero) ë°©ì§€\n",
    "    iou = intersection / (union + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def get_boundary(mask, dilation_pixels=2):\n",
    "    \"\"\"\n",
    "    ì´ì§„ ë§ˆìŠ¤í¬(binary mask)ì—ì„œ ê²½ê³„ì„  ì˜ì—­ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    mask = mask.astype(np.uint8)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    # ì¹¨ì‹ ì—°ì‚°(Erosion)ì„ í†µí•´ ë‚´ë¶€ë¥¼ ê¹ì•„ëƒ„\n",
    "    eroded = cv2.erode(mask, kernel, iterations=dilation_pixels)\n",
    "    # ì›ë³¸ ë§ˆìŠ¤í¬ì—ì„œ ì¹¨ì‹ëœ ë§ˆìŠ¤í¬ë¥¼ ë¹¼ì„œ ê²½ê³„ì„ ë§Œ ë‚¨ê¹€\n",
    "    boundary = mask - eroded\n",
    "    return boundary\n",
    "\n",
    "def compute_boundary_iou(preds, targets, num_classes, dilation_pixels=2):\n",
    "    \"\"\"\n",
    "    ê° í´ë˜ìŠ¤ë³„ë¡œ Boundary IoUë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    preds_np = preds.cpu().numpy()\n",
    "    targets_np = targets.cpu().numpy()\n",
    "    b_ious = []\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        class_preds = (preds_np == c)\n",
    "        class_targets = (targets_np == c)\n",
    "\n",
    "        ious_per_batch = []\n",
    "        for i in range(preds_np.shape[0]): # ë°°ì¹˜ ë‚´ ê° ì´ë¯¸ì§€ë³„ë¡œ ë°˜ë³µ\n",
    "            gt_boundary = get_boundary(class_targets[i], dilation_pixels)\n",
    "            pred_boundary = get_boundary(class_preds[i], dilation_pixels)\n",
    "\n",
    "            intersection = ((gt_boundary > 0) & (pred_boundary > 0)).sum()\n",
    "            union = ((gt_boundary > 0) | (pred_boundary > 0)).sum()\n",
    "\n",
    "            if union == 0:\n",
    "                # ì •ë‹µ(GT)ê³¼ ì˜ˆì¸¡ ëª¨ë‘ í•´ë‹¹ í´ë˜ìŠ¤ì˜ ê²½ê³„ì„ ì´ ì—†ëŠ” ê²½ìš°, \n",
    "                # ì™„ë²½í•˜ê²Œ ì¼ì¹˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ IoU 1.0 ë¶€ì—¬\n",
    "                # (íŠ¹ì • í´ë˜ìŠ¤ê°€ GTì— ì—†ì„ ë•Œ ì „ì²´ í‰ê· ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³ ë ¤í•œ ì²˜ë¦¬)\n",
    "                ious_per_batch.append(1.0)\n",
    "            else:\n",
    "                ious_per_batch.append(intersection / union)\n",
    "\n",
    "        # ë°°ì¹˜ ë‚´ì˜ ì–´ë–¤ ì´ë¯¸ì§€ì—ì„œë„ í•´ë‹¹ í´ë˜ìŠ¤ì˜ ê²½ê³„ì„ ì´ ë°œê²¬ë˜ì§€ ì•Šì€ ê²½ìš°\n",
    "        if not ious_per_batch:\n",
    "            b_ious.append(0.0) # í•„ìš”ì— ë”°ë¼ NaN ë“±ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥\n",
    "        else:\n",
    "            b_ious.append(np.mean(ious_per_batch))\n",
    "\n",
    "    return np.array(b_ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffcccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš€ SegFormer-B2 Evaluation Start...\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "total_category_ious = np.zeros(CFG_EVAL['num_classes'])\n",
    "total_boundary_ious = np.zeros(CFG_EVAL['num_classes'])\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    eval_pbar = tqdm(range(0, len(img_ids), CFG['batch_size']), desc=\"Evaluating\")\n",
    "    for i in eval_pbar:\n",
    "        batch_ids = img_ids[i : i + CFG['batch_size']]\n",
    "\n",
    "        images, masks = [], []\n",
    "        for img_id in batch_ids:\n",
    "            img, msk = process_single_data(coco, img_id, IMG_DIR, id_to_idx, train_transform)\n",
    "            images.append(img)\n",
    "            masks.append(msk)\n",
    "\n",
    "        X = torch.stack(images).to(CFG['device']).contiguous()\n",
    "        y = torch.stack(masks).to(CFG['device']).contiguous()\n",
    "\n",
    "        outputs = model(X).logits.contiguous()\n",
    "        upsampled_logits = nn.functional.interpolate(\n",
    "            outputs,\n",
    "            size=y.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        ).contiguous()\n",
    "\n",
    "        preds = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        # í•´ë‹¹ batchì˜ IoU ê³„ì‚° \n",
    "        category_ious = compute_category_iou(preds, y, CFG_EVAL['num_classes'])\n",
    "        boundary_ious = compute_boundary_iou(preds, y, CFG_EVAL['num_classes'])\n",
    "\n",
    "        total_category_ious += category_ious\n",
    "        total_boundary_ious += boundary_ious\n",
    "        num_batches += 1\n",
    "\n",
    "# í‰ê·  IoUs ê³„ì‚°\n",
    "mean_category_ious = total_category_ious / num_batches\n",
    "mean_boundary_ious = total_boundary_ious / num_batches\n",
    "\n",
    "print()\n",
    "print(\"--- Evaluation Results ---\")\n",
    "print(\"Category-specific IoU (Mean over batches):\")\n",
    "for i, val in enumerate(mean_category_ious):\n",
    "    print(f\"  Class {i} ({id2label[i]}): {val:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Boundary IoU (Mean over batches):\")\n",
    "for i, val in enumerate(mean_boundary_ious):\n",
    "    print(f\"  Class {i} ({id2label[i]}): {val:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"Mean Category IoU (mIoU):\", np.mean(mean_category_ious))\n",
    "print(\"Mean Boundary IoU (mBoU):\", np.mean(mean_boundary_ious))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc13abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : requirement.txtë¡œ ë³´ë‚´ì\n",
    "import sys\n",
    "!{sys.executable} -m pip install thop\n",
    "\n",
    "print(\"thop installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from thop import profile\n",
    "\n",
    "print(\"ğŸš€ Starting Model Profiling...\")\n",
    "\n",
    "# 2. ëª¨ë¸ì„ ìœ„í•œ ë”ë¯¸ ì…ë ¥ í…ì„œ ìƒì„±\n",
    "dummy_input = torch.randn(1, 3, CFG['img_size'][0], CFG['img_size'][1]).to(CFG['device'])\n",
    "\n",
    "# 3. ë”ë¯¸ ì…ë ¥ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ MACs ë° ë§¤ê°œë³€ìˆ˜(params)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ thop.profile ì‚¬ìš©. verbose=Falseë¡œ ì„¤ì •.\n",
    "macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "\n",
    "# 4. ê³„ì‚°ëœ MACsë¥¼ 1e9ë¡œ ë‚˜ëˆ„ì–´ GFLOPsë¡œ ë³€í™˜.\n",
    "gflops = macs / 1e9\n",
    "\n",
    "print(f\"Model Parameters (M): {params / 1e6:.2f}\")\n",
    "print(f\"MACs (G): {macs / 1e9:.2f}\")\n",
    "print(f\"GFLOPs: {gflops:.2f}\")\n",
    "\n",
    "# 5. ì§€ì—° ì‹œê°„ ë° FPS ì¸¡ì •ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”.\n",
    "num_warmup_runs = 10\n",
    "num_inference_runs = 100\n",
    "total_latency = 0.0\n",
    "\n",
    "# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "model.eval()\n",
    "\n",
    "# 6. ì›Œë°ì—… ì‹¤í–‰ ìˆ˜í–‰\n",
    "print(f\"Performing {num_warmup_runs} warm-up runs...\")\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_warmup_runs):\n",
    "        _ = model(dummy_input)\n",
    "\n",
    "# 7. ì¶”ë¡  ì§€ì—° ì‹œê°„ ì¸¡ì •\n",
    "print(f\"Measuring latency over {num_inference_runs} inference runs...\")\n",
    "with torch.no_grad():\n",
    "    for _ in range(num_inference_runs):\n",
    "        start_time = time.perf_counter()\n",
    "        _ = model(dummy_input)\n",
    "        end_time = time.perf_counter()\n",
    "        total_latency += (end_time - start_time)\n",
    "\n",
    "# 8. í‰ê·  ì§€ì—° ì‹œê°„(ms) ê³„ì‚°\n",
    "average_latency_ms = (total_latency / num_inference_runs) * 1000\n",
    "\n",
    "# 9. FPS ê³„ì‚°\n",
    "fps = 1000 / average_latency_ms\n",
    "\n",
    "print(f\"Average Inference Latency: {average_latency_ms:.2f} ms\")\n",
    "print(f\"Frames Per Second (FPS): {fps:.2f}\")\n",
    "print(\"Model Profiling Complete.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aipel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
